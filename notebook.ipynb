{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pathlib\n",
    "\n",
    "ROOT = pathlib.Path(\"/content/drive/MyDrive/Colab Notebooks/KanjiLookup\")\n",
    "\n",
    "GENERATED = ROOT / \"generated\"\n",
    "\n",
    "INPUT_FONTS_FOLDER = ROOT / 'fonts'\n",
    "INPUT_KANJI_FOLDER = ROOT / 'text'\n",
    "\n",
    "GENERATED_EMBEDDINGS_FOLDER = GENERATED / 'embeddings'\n",
    "\n",
    "MODEL = \"kha-white/manga-ocr-base\"\n",
    "\n",
    "MODEL_EMBEDDING_SIZE = 768\n",
    "MODEL_IMAGE_SIZE = 224\n",
    "FONT_SIZE = 184"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from PIL import Image, ImageFont, ImageDraw\n",
    "\n",
    "def get_standard_kanji_set() -> set[str]:\n",
    "    file = INPUT_KANJI_FOLDER / \"kanji_joyo.txt\"\n",
    "    return set(file.read_text(encoding=\"UTF-8\").splitlines())\n",
    "\n",
    "def load_kanji_list() -> list[str]:\n",
    "    kanji_list = []\n",
    "    for file in INPUT_KANJI_FOLDER.glob(\"*.txt\"):\n",
    "        kanji_list += file.read_text(encoding=\"UTF-8\").splitlines()\n",
    "    return kanji_list\n",
    "\n",
    "\n",
    "def draw_kanji(font: ImageFont.FreeTypeFont, kanji: str):\n",
    "    \"Create an image with the given `kanji` drawn in the given `font`\"\n",
    "    image = Image.new('L', (MODEL_IMAGE_SIZE, MODEL_IMAGE_SIZE), color=255)\n",
    "    draw = ImageDraw.Draw(image)\n",
    "    draw.text((MODEL_IMAGE_SIZE // 2, MODEL_IMAGE_SIZE // 2), kanji, font=font, anchor=\"mm\", fill=0)\n",
    "    return image\n",
    "\n",
    "def check_has_text(image: Image):\n",
    "    \"Verifies if the image contain anything at all\"\n",
    "    _arr = np.asarray(image)\n",
    "    if _arr.min() == _arr.max():\n",
    "        return False\n",
    "    return True\n",
    "\n",
    "\n",
    "def list_fonts() -> dict[str, ImageFont.FreeTypeFont]:\n",
    "    \"\"\"Returns a dictionary of `font_name -> ImageFont`\"\"\"\n",
    "    return {\n",
    "        # font_file.stem: ImageFont.truetype(font_file, FONT_SIZE)\n",
    "        # ^ Worked on my machine, but not on colab.\n",
    "        font_file.stem: ImageFont.truetype(str(font_file), FONT_SIZE)\n",
    "        for font_file in INPUT_FONTS_FOLDER.glob(\"**/*.ttf\")\n",
    "    }\n",
    "\n",
    "def generate_images_for_font(font: ImageFont.FreeTypeFont, kanji_list: list[str]) -> dict[str, Image.Image]:\n",
    "    \"\"\"Returns a dictionary of `kanji -> Image`\"\"\"\n",
    "    out = {}\n",
    "    _bad = []\n",
    "    for kanji in kanji_list:\n",
    "        image = draw_kanji(font, kanji)\n",
    "        if check_has_text(image):\n",
    "            out[kanji] = image\n",
    "        else:\n",
    "            _bad.append(kanji)\n",
    "    if _bad:\n",
    "        print(f\"Font {font.getname()} does not seems to support {_bad} characters, skipping them for this font\")\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "import torch\n",
    "from transformers import (\n",
    "    VisionEncoderDecoderModel,\n",
    "    ViTImageProcessor,  # Load extractor\n",
    "    ViTModel,  # Load ViT encoder\n",
    ")\n",
    "\n",
    "assert MODEL == \"kha-white/manga-ocr-base\", \"Other models are not natively supported, \\\n",
    "    you may have to change a lot of things to get it to work\"\n",
    "assert MODEL_EMBEDDING_SIZE == 768, \"The only model embedding size supported is 768\"\n",
    "\n",
    "def load_model() -> tuple[ViTImageProcessor, ViTModel]:\n",
    "    \"\"\"Load the model based on the config.py file.\n",
    "    Returns the `feature_extractor` and the `encoder`, in this order.\n",
    "    \"\"\"\n",
    "    print(\"Loading Image Processor from HuggingFace Hub\")\n",
    "    feature_extractor: ViTImageProcessor = ViTImageProcessor.from_pretrained(MODEL, requires_grad=False)\n",
    "\n",
    "    print(\"Loading ViT Model from HuggingFace Hub\")\n",
    "    model: ViTModel = VisionEncoderDecoderModel.from_pretrained(MODEL).encoder\n",
    "\n",
    "    if torch.cuda.is_available():\n",
    "        print('Using CUDA')\n",
    "        model.cuda()\n",
    "    else:\n",
    "        print('Using CPU')\n",
    "\n",
    "    return feature_extractor, model\n",
    "\n",
    "\n",
    "def get_embeddings(feature_extractor: ViTImageProcessor, encoder: ViTModel, images: list[Image.Image]) -> torch.Tensor:\n",
    "    \"\"\"Processes the images and returns their Embeddings\"\"\"\n",
    "    images_rgb = [image.convert(\"RGB\") for image in images]\n",
    "    with torch.inference_mode():\n",
    "        pixel_values: torch.Tensor = feature_extractor(images_rgb, return_tensors=\"pt\")[\"pixel_values\"]\n",
    "        return encoder(pixel_values.to(encoder.device))[\"pooler_output\"].cpu()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import typing\n",
    "\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "\n",
    "T = typing.TypeVar(\"T\")\n",
    "\n",
    "BATCH_SIZE = 1024\n",
    "\n",
    "def batched(original: list[T], group_size: int) -> list[list[T]]:\n",
    "    groups = []\n",
    "    for i in range(0, len(original), group_size):\n",
    "        groups.append(original[i : i + group_size])\n",
    "    # return [groups[0]]  # Return only the first group of each batch for testing\n",
    "    return groups\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "extractor, encoder = load_model()\n",
    "\n",
    "fonts = list_fonts()\n",
    "print(f\"Using the {len(fonts)} following fonts: {fonts.keys()}\")\n",
    "\n",
    "kanji_list = load_kanji_list()\n",
    "kanji_batches = batched(kanji_list, BATCH_SIZE)\n",
    "print(f\"Processing a total of {len(kanji_list)} Kanji in {len(kanji_batches)} batches of {BATCH_SIZE}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for font_name in tqdm(fonts):\n",
    "    font = fonts[font_name]\n",
    "    font_embeddings_folder = GENERATED_EMBEDDINGS_FOLDER / font_name\n",
    "    font_embeddings_folder.mkdir(exist_ok=True, parents=True)\n",
    "\n",
    "    print(f\"Generating Embeddings for font {font.getname()}\")\n",
    "\n",
    "    for batch_index, kanji_batch in enumerate(tqdm(kanji_batches)):\n",
    "        images_dict = generate_images_for_font(font, kanji_batch)\n",
    "\n",
    "        if len(images_dict) == 0:\n",
    "            # The font does not supports any of the characters from the batch\n",
    "            print(f\"The font {font_name} did not support any of the characters for batch {batch_index}\")\n",
    "            continue\n",
    "\n",
    "        tensor_out_file = font_embeddings_folder / f\"batch_{batch_index}.pt\"\n",
    "        labels_out_file = font_embeddings_folder / f\"batch_{batch_index}.txt\"\n",
    "\n",
    "        labels = \"\\n\".join(images_dict.keys())  # We cannot use kanji_batch directly\n",
    "        # because `generate_images_for_font` may skip some characters\n",
    "        tensor = get_embeddings(extractor, encoder, list(images_dict.values()))\n",
    "\n",
    "        labels_out_file.write_text(labels, encoding=\"UTF-8\")\n",
    "        torch.save(tensor, tensor_out_file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
